{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/intelligenxe/FinRL-type/blob/main/FinRL_Train.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "These codes are based on the codes of the following videos series by linkedin.com/in/everett-minshall/ : <br>\n",
        "https://www.youtube.com/watch?v=OrqffjqkklM  <br>\n",
        "https://www.youtube.com/watch?v=uYC3sc5gers <br>\n",
        "https://www.youtube.com/watch?v=335QxO36pzo&t=15s <br>\n",
        "The codes in the above videos are in turn based on https://github.com/AI4Finance-Foundation/FinRL?tab=readme-ov-file#tutorials (FinRL Tutorials)."
      ],
      "metadata": {
        "id": "sU-85IDBXw9m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup installs & imports"
      ],
      "metadata": {
        "id": "fLCIwnUjhfCb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hv51xG1BhAi7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3d74c6d7-1724-4d12-89cb-ad69c14e9f09"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting swig\n",
            "  Downloading swig-4.1.1.post1-py2.py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: swig\n",
            "Successfully installed swig-4.1.1.post1\n",
            "Collecting wrds\n",
            "  Downloading wrds-3.1.6-py3-none-any.whl (12 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from wrds) (1.23.5)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from wrds) (1.5.3)\n",
            "Collecting psycopg2-binary (from wrds)\n",
            "  Downloading psycopg2_binary-2.9.9-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from wrds) (1.11.4)\n",
            "Collecting sqlalchemy<2 (from wrds)\n",
            "  Downloading SQLAlchemy-1.4.51-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m36.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from sqlalchemy<2->wrds) (3.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->wrds) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->wrds) (2023.3.post1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->wrds) (1.16.0)\n",
            "Installing collected packages: sqlalchemy, psycopg2-binary, wrds\n",
            "  Attempting uninstall: sqlalchemy\n",
            "    Found existing installation: SQLAlchemy 2.0.23\n",
            "    Uninstalling SQLAlchemy-2.0.23:\n",
            "      Successfully uninstalled SQLAlchemy-2.0.23\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "ipython-sql 0.5.0 requires sqlalchemy>=2.0, but you have sqlalchemy 1.4.51 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed psycopg2-binary-2.9.9 sqlalchemy-1.4.51 wrds-3.1.6\n",
            "Collecting pyportfolioopt\n",
            "  Downloading pyportfolioopt-1.5.5-py3-none-any.whl (61 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.9/61.9 kB\u001b[0m \u001b[31m992.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: cvxpy<2.0.0,>=1.1.19 in /usr/local/lib/python3.10/dist-packages (from pyportfolioopt) (1.3.2)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from pyportfolioopt) (1.23.5)\n",
            "Requirement already satisfied: pandas>=0.19 in /usr/local/lib/python3.10/dist-packages (from pyportfolioopt) (1.5.3)\n",
            "Requirement already satisfied: scipy<2.0,>=1.3 in /usr/local/lib/python3.10/dist-packages (from pyportfolioopt) (1.11.4)\n",
            "Requirement already satisfied: osqp>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from cvxpy<2.0.0,>=1.1.19->pyportfolioopt) (0.6.2.post8)\n",
            "Requirement already satisfied: ecos>=2 in /usr/local/lib/python3.10/dist-packages (from cvxpy<2.0.0,>=1.1.19->pyportfolioopt) (2.0.12)\n",
            "Requirement already satisfied: scs>=1.1.6 in /usr/local/lib/python3.10/dist-packages (from cvxpy<2.0.0,>=1.1.19->pyportfolioopt) (3.2.4.post1)\n",
            "Requirement already satisfied: setuptools>65.5.1 in /usr/local/lib/python3.10/dist-packages (from cvxpy<2.0.0,>=1.1.19->pyportfolioopt) (67.7.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.19->pyportfolioopt) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.19->pyportfolioopt) (2023.3.post1)\n",
            "Requirement already satisfied: qdldl in /usr/local/lib/python3.10/dist-packages (from osqp>=0.4.1->cvxpy<2.0.0,>=1.1.19->pyportfolioopt) (0.1.7.post0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas>=0.19->pyportfolioopt) (1.16.0)\n",
            "Installing collected packages: pyportfolioopt\n",
            "Successfully installed pyportfolioopt-1.5.5\n",
            "Collecting git+https://github.com/AI4Finance-Foundation/FinRL.git\n",
            "  Cloning https://github.com/AI4Finance-Foundation/FinRL.git to /tmp/pip-req-build-gysm9lf7\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/AI4Finance-Foundation/FinRL.git /tmp/pip-req-build-gysm9lf7\n",
            "  Resolved https://github.com/AI4Finance-Foundation/FinRL.git to commit 754bd1bb560a90d98494e116e734998353f02a34\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting elegantrl@ git+https://github.com/AI4Finance-Foundation/ElegantRL.git#egg=elegantrl (from finrl==0.3.6)\n",
            "  Cloning https://github.com/AI4Finance-Foundation/ElegantRL.git to /tmp/pip-install-u5js2_w6/elegantrl_461db35d56c94a0f9666b875ff1240fe\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/AI4Finance-Foundation/ElegantRL.git /tmp/pip-install-u5js2_w6/elegantrl_461db35d56c94a0f9666b875ff1240fe\n",
            "  Resolved https://github.com/AI4Finance-Foundation/ElegantRL.git to commit b4b9d662b9f9cb7cc368ac2b1036b5119eb20be4\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting alpaca-trade-api<4,>=3 (from finrl==0.3.6)\n",
            "  Downloading alpaca_trade_api-3.0.2-py3-none-any.whl (34 kB)\n",
            "Collecting ccxt<4,>=3 (from finrl==0.3.6)\n",
            "  Downloading ccxt-3.1.60-py2.py3-none-any.whl (4.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.0/4.0 MB\u001b[0m \u001b[31m16.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting exchange-calendars<5,>=4 (from finrl==0.3.6)\n",
            "  Downloading exchange_calendars-4.5.1-py3-none-any.whl (191 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m191.3/191.3 kB\u001b[0m \u001b[31m26.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting jqdatasdk<2,>=1 (from finrl==0.3.6)\n",
            "  Downloading jqdatasdk-1.9.2-py3-none-any.whl (167 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m167.1/167.1 kB\u001b[0m \u001b[31m23.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pyfolio<0.10,>=0.9 (from finrl==0.3.6)\n",
            "  Downloading pyfolio-0.9.2.tar.gz (91 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m91.1/91.1 kB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: pyportfolioopt<2,>=1 in /usr/local/lib/python3.10/dist-packages (from finrl==0.3.6) (1.5.5)\n",
            "Collecting ray[default,tune]<3,>=2 (from finrl==0.3.6)\n",
            "  Downloading ray-2.9.0-cp310-cp310-manylinux2014_x86_64.whl (64.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.9/64.9 MB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scikit-learn<2,>=1 in /usr/local/lib/python3.10/dist-packages (from finrl==0.3.6) (1.2.2)\n",
            "Collecting stable-baselines3[extra]>=2.0.0a5 (from finrl==0.3.6)\n",
            "  Downloading stable_baselines3-2.2.1-py3-none-any.whl (181 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m181.7/181.7 kB\u001b[0m \u001b[31m25.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting stockstats<0.6,>=0.5 (from finrl==0.3.6)\n",
            "  Downloading stockstats-0.5.4-py2.py3-none-any.whl (21 kB)\n",
            "Requirement already satisfied: wrds<4,>=3 in /usr/local/lib/python3.10/dist-packages (from finrl==0.3.6) (3.1.6)\n",
            "Requirement already satisfied: yfinance<0.3,>=0.2 in /usr/local/lib/python3.10/dist-packages (from finrl==0.3.6) (0.2.33)\n",
            "Requirement already satisfied: pandas>=0.18.1 in /usr/local/lib/python3.10/dist-packages (from alpaca-trade-api<4,>=3->finrl==0.3.6) (1.5.3)\n",
            "Requirement already satisfied: numpy>=1.11.1 in /usr/local/lib/python3.10/dist-packages (from alpaca-trade-api<4,>=3->finrl==0.3.6) (1.23.5)\n",
            "Requirement already satisfied: requests<3,>2 in /usr/local/lib/python3.10/dist-packages (from alpaca-trade-api<4,>=3->finrl==0.3.6) (2.31.0)\n",
            "Collecting urllib3<2,>1.24 (from alpaca-trade-api<4,>=3->finrl==0.3.6)\n",
            "  Downloading urllib3-1.26.18-py2.py3-none-any.whl (143 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.8/143.8 kB\u001b[0m \u001b[31m21.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: websocket-client<2,>=0.56.0 in /usr/local/lib/python3.10/dist-packages (from alpaca-trade-api<4,>=3->finrl==0.3.6) (1.7.0)\n",
            "Collecting websockets<11,>=9.0 (from alpaca-trade-api<4,>=3->finrl==0.3.6)\n",
            "  Downloading websockets-10.4-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (106 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.8/106.8 kB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting msgpack==1.0.3 (from alpaca-trade-api<4,>=3->finrl==0.3.6)\n",
            "  Downloading msgpack-1.0.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (323 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m323.7/323.7 kB\u001b[0m \u001b[31m39.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting aiohttp==3.8.2 (from alpaca-trade-api<4,>=3->finrl==0.3.6)\n",
            "  Downloading aiohttp-3.8.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m69.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting PyYAML==6.0 (from alpaca-trade-api<4,>=3->finrl==0.3.6)\n",
            "  Downloading PyYAML-6.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (682 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m682.2/682.2 kB\u001b[0m \u001b[31m53.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting deprecation==2.1.0 (from alpaca-trade-api<4,>=3->finrl==0.3.6)\n",
            "  Downloading deprecation-2.1.0-py2.py3-none-any.whl (11 kB)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp==3.8.2->alpaca-trade-api<4,>=3->finrl==0.3.6) (23.1.0)\n",
            "Collecting charset-normalizer<3.0,>=2.0 (from aiohttp==3.8.2->alpaca-trade-api<4,>=3->finrl==0.3.6)\n",
            "  Downloading charset_normalizer-2.1.1-py3-none-any.whl (39 kB)\n",
            "Collecting multidict<6.0,>=4.5 (from aiohttp==3.8.2->alpaca-trade-api<4,>=3->finrl==0.3.6)\n",
            "  Downloading multidict-5.2.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (175 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m175.1/175.1 kB\u001b[0m \u001b[31m21.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp==3.8.2->alpaca-trade-api<4,>=3->finrl==0.3.6) (4.0.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp==3.8.2->alpaca-trade-api<4,>=3->finrl==0.3.6) (1.9.4)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp==3.8.2->alpaca-trade-api<4,>=3->finrl==0.3.6) (1.4.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp==3.8.2->alpaca-trade-api<4,>=3->finrl==0.3.6) (1.3.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from deprecation==2.1.0->alpaca-trade-api<4,>=3->finrl==0.3.6) (23.2)\n",
            "Requirement already satisfied: setuptools>=60.9.0 in /usr/local/lib/python3.10/dist-packages (from ccxt<4,>=3->finrl==0.3.6) (67.7.2)\n",
            "Requirement already satisfied: certifi>=2018.1.18 in /usr/local/lib/python3.10/dist-packages (from ccxt<4,>=3->finrl==0.3.6) (2023.11.17)\n",
            "Requirement already satisfied: cryptography>=2.6.1 in /usr/local/lib/python3.10/dist-packages (from ccxt<4,>=3->finrl==0.3.6) (41.0.7)\n",
            "Collecting aiodns>=1.1.1 (from ccxt<4,>=3->finrl==0.3.6)\n",
            "  Downloading aiodns-3.1.1-py3-none-any.whl (5.4 kB)\n",
            "Collecting pyluach (from exchange-calendars<5,>=4->finrl==0.3.6)\n",
            "  Downloading pyluach-2.2.0-py3-none-any.whl (25 kB)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.10/dist-packages (from exchange-calendars<5,>=4->finrl==0.3.6) (2.8.2)\n",
            "Requirement already satisfied: toolz in /usr/local/lib/python3.10/dist-packages (from exchange-calendars<5,>=4->finrl==0.3.6) (0.12.0)\n",
            "Collecting tzdata (from exchange-calendars<5,>=4->finrl==0.3.6)\n",
            "  Downloading tzdata-2023.4-py2.py3-none-any.whl (346 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m346.6/346.6 kB\u001b[0m \u001b[31m35.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting korean-lunar-calendar (from exchange-calendars<5,>=4->finrl==0.3.6)\n",
            "  Downloading korean_lunar_calendar-0.3.1-py3-none-any.whl (9.0 kB)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from jqdatasdk<2,>=1->finrl==0.3.6) (1.16.0)\n",
            "Requirement already satisfied: SQLAlchemy>=1.2.8 in /usr/local/lib/python3.10/dist-packages (from jqdatasdk<2,>=1->finrl==0.3.6) (1.4.51)\n",
            "Collecting thriftpy2>=0.3.9 (from jqdatasdk<2,>=1->finrl==0.3.6)\n",
            "  Downloading thriftpy2-0.4.17.tar.gz (519 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m519.4/519.4 kB\u001b[0m \u001b[31m46.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pymysql>=0.7.6 (from jqdatasdk<2,>=1->finrl==0.3.6)\n",
            "  Downloading PyMySQL-1.1.0-py3-none-any.whl (44 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.8/44.8 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: ipython>=3.2.3 in /usr/local/lib/python3.10/dist-packages (from pyfolio<0.10,>=0.9->finrl==0.3.6) (7.34.0)\n",
            "Requirement already satisfied: matplotlib>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from pyfolio<0.10,>=0.9->finrl==0.3.6) (3.7.1)\n",
            "Requirement already satisfied: pytz>=2014.10 in /usr/local/lib/python3.10/dist-packages (from pyfolio<0.10,>=0.9->finrl==0.3.6) (2023.3.post1)\n",
            "Requirement already satisfied: scipy>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from pyfolio<0.10,>=0.9->finrl==0.3.6) (1.11.4)\n",
            "Requirement already satisfied: seaborn>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from pyfolio<0.10,>=0.9->finrl==0.3.6) (0.12.2)\n",
            "Collecting empyrical>=0.5.0 (from pyfolio<0.10,>=0.9->finrl==0.3.6)\n",
            "  Downloading empyrical-0.5.5.tar.gz (52 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.8/52.8 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: cvxpy<2.0.0,>=1.1.19 in /usr/local/lib/python3.10/dist-packages (from pyportfolioopt<2,>=1->finrl==0.3.6) (1.3.2)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.10/dist-packages (from ray[default,tune]<3,>=2->finrl==0.3.6) (8.1.7)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from ray[default,tune]<3,>=2->finrl==0.3.6) (3.13.1)\n",
            "Requirement already satisfied: jsonschema in /usr/local/lib/python3.10/dist-packages (from ray[default,tune]<3,>=2->finrl==0.3.6) (4.19.2)\n",
            "Requirement already satisfied: protobuf!=3.19.5,>=3.15.3 in /usr/local/lib/python3.10/dist-packages (from ray[default,tune]<3,>=2->finrl==0.3.6) (3.20.3)\n",
            "Collecting aiohttp-cors (from ray[default,tune]<3,>=2->finrl==0.3.6)\n",
            "  Downloading aiohttp_cors-0.7.0-py3-none-any.whl (27 kB)\n",
            "Collecting colorful (from ray[default,tune]<3,>=2->finrl==0.3.6)\n",
            "  Downloading colorful-0.5.5-py2.py3-none-any.whl (201 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m201.4/201.4 kB\u001b[0m \u001b[31m26.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting py-spy>=0.2.0 (from ray[default,tune]<3,>=2->finrl==0.3.6)\n",
            "  Downloading py_spy-0.3.14-py2.py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.whl (3.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m99.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting gpustat>=1.0.0 (from ray[default,tune]<3,>=2->finrl==0.3.6)\n",
            "  Downloading gpustat-1.1.1.tar.gz (98 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.1/98.1 kB\u001b[0m \u001b[31m15.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting opencensus (from ray[default,tune]<3,>=2->finrl==0.3.6)\n",
            "  Downloading opencensus-0.11.4-py2.py3-none-any.whl (128 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m128.2/128.2 kB\u001b[0m \u001b[31m20.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,<3 in /usr/local/lib/python3.10/dist-packages (from ray[default,tune]<3,>=2->finrl==0.3.6) (1.10.13)\n",
            "Requirement already satisfied: prometheus-client>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from ray[default,tune]<3,>=2->finrl==0.3.6) (0.19.0)\n",
            "Requirement already satisfied: smart-open in /usr/local/lib/python3.10/dist-packages (from ray[default,tune]<3,>=2->finrl==0.3.6) (6.4.0)\n",
            "Collecting virtualenv<20.21.1,>=20.0.24 (from ray[default,tune]<3,>=2->finrl==0.3.6)\n",
            "  Downloading virtualenv-20.21.0-py3-none-any.whl (8.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.7/8.7 MB\u001b[0m \u001b[31m55.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: grpcio>=1.42.0 in /usr/local/lib/python3.10/dist-packages (from ray[default,tune]<3,>=2->finrl==0.3.6) (1.60.0)\n",
            "Collecting tensorboardX>=1.9 (from ray[default,tune]<3,>=2->finrl==0.3.6)\n",
            "  Downloading tensorboardX-2.6.2.2-py2.py3-none-any.whl (101 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.7/101.7 kB\u001b[0m \u001b[31m16.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyarrow>=6.0.1 in /usr/local/lib/python3.10/dist-packages (from ray[default,tune]<3,>=2->finrl==0.3.6) (10.0.1)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from ray[default,tune]<3,>=2->finrl==0.3.6) (2023.6.0)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn<2,>=1->finrl==0.3.6) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn<2,>=1->finrl==0.3.6) (3.2.0)\n",
            "Collecting gymnasium<0.30,>=0.28.1 (from stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6)\n",
            "  Downloading gymnasium-0.29.1-py3-none-any.whl (953 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m953.9/953.9 kB\u001b[0m \u001b[31m80.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch>=1.13 in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (2.1.0+cu121)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (2.2.1)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (4.8.0.76)\n",
            "Requirement already satisfied: pygame in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (2.5.2)\n",
            "Requirement already satisfied: tensorboard>=2.9.1 in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (2.15.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (5.9.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (4.66.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (13.7.0)\n",
            "Collecting shimmy[atari]~=1.3.0 (from stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6)\n",
            "  Downloading Shimmy-1.3.0-py3-none-any.whl (37 kB)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (9.4.0)\n",
            "Collecting autorom[accept-rom-license]~=0.6.1 (from stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6)\n",
            "  Downloading AutoROM-0.6.1-py3-none-any.whl (9.4 kB)\n",
            "Requirement already satisfied: psycopg2-binary in /usr/local/lib/python3.10/dist-packages (from wrds<4,>=3->finrl==0.3.6) (2.9.9)\n",
            "Requirement already satisfied: multitasking>=0.0.7 in /usr/local/lib/python3.10/dist-packages (from yfinance<0.3,>=0.2->finrl==0.3.6) (0.0.11)\n",
            "Requirement already satisfied: lxml>=4.9.1 in /usr/local/lib/python3.10/dist-packages (from yfinance<0.3,>=0.2->finrl==0.3.6) (4.9.4)\n",
            "Requirement already satisfied: appdirs>=1.4.4 in /usr/local/lib/python3.10/dist-packages (from yfinance<0.3,>=0.2->finrl==0.3.6) (1.4.4)\n",
            "Requirement already satisfied: frozendict>=2.3.4 in /usr/local/lib/python3.10/dist-packages (from yfinance<0.3,>=0.2->finrl==0.3.6) (2.3.10)\n",
            "Requirement already satisfied: peewee>=3.16.2 in /usr/local/lib/python3.10/dist-packages (from yfinance<0.3,>=0.2->finrl==0.3.6) (3.17.0)\n",
            "Requirement already satisfied: beautifulsoup4>=4.11.1 in /usr/local/lib/python3.10/dist-packages (from yfinance<0.3,>=0.2->finrl==0.3.6) (4.11.2)\n",
            "Requirement already satisfied: html5lib>=1.1 in /usr/local/lib/python3.10/dist-packages (from yfinance<0.3,>=0.2->finrl==0.3.6) (1.1)\n",
            "Requirement already satisfied: gym in /usr/local/lib/python3.10/dist-packages (from elegantrl@ git+https://github.com/AI4Finance-Foundation/ElegantRL.git#egg=elegantrl->finrl==0.3.6) (0.25.2)\n",
            "Collecting pycares>=4.0.0 (from aiodns>=1.1.1->ccxt<4,>=3->finrl==0.3.6)\n",
            "  Downloading pycares-4.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (288 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m288.7/288.7 kB\u001b[0m \u001b[31m39.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting AutoROM.accept-rom-license (from autorom[accept-rom-license]~=0.6.1->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6)\n",
            "  Downloading AutoROM.accept-rom-license-0.6.1.tar.gz (434 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m434.7/434.7 kB\u001b[0m \u001b[31m47.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4>=4.11.1->yfinance<0.3,>=0.2->finrl==0.3.6) (2.5)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography>=2.6.1->ccxt<4,>=3->finrl==0.3.6) (1.16.0)\n",
            "Requirement already satisfied: osqp>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from cvxpy<2.0.0,>=1.1.19->pyportfolioopt<2,>=1->finrl==0.3.6) (0.6.2.post8)\n",
            "Requirement already satisfied: ecos>=2 in /usr/local/lib/python3.10/dist-packages (from cvxpy<2.0.0,>=1.1.19->pyportfolioopt<2,>=1->finrl==0.3.6) (2.0.12)\n",
            "Requirement already satisfied: scs>=1.1.6 in /usr/local/lib/python3.10/dist-packages (from cvxpy<2.0.0,>=1.1.19->pyportfolioopt<2,>=1->finrl==0.3.6) (3.2.4.post1)\n",
            "Requirement already satisfied: pandas-datareader>=0.2 in /usr/local/lib/python3.10/dist-packages (from empyrical>=0.5.0->pyfolio<0.10,>=0.9->finrl==0.3.6) (0.10.0)\n",
            "Collecting nvidia-ml-py>=11.450.129 (from gpustat>=1.0.0->ray[default,tune]<3,>=2->finrl==0.3.6)\n",
            "  Downloading nvidia_ml_py-12.535.133-py3-none-any.whl (37 kB)\n",
            "Collecting blessed>=1.17.1 (from gpustat>=1.0.0->ray[default,tune]<3,>=2->finrl==0.3.6)\n",
            "  Downloading blessed-1.20.0-py2.py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.4/58.4 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium<0.30,>=0.28.1->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (4.5.0)\n",
            "Collecting farama-notifications>=0.0.1 (from gymnasium<0.30,>=0.28.1->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6)\n",
            "  Using cached Farama_Notifications-0.0.4-py3-none-any.whl (2.5 kB)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from html5lib>=1.1->yfinance<0.3,>=0.2->finrl==0.3.6) (0.5.1)\n",
            "Collecting jedi>=0.16 (from ipython>=3.2.3->pyfolio<0.10,>=0.9->finrl==0.3.6)\n",
            "  Downloading jedi-0.19.1-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m65.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from ipython>=3.2.3->pyfolio<0.10,>=0.9->finrl==0.3.6) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.10/dist-packages (from ipython>=3.2.3->pyfolio<0.10,>=0.9->finrl==0.3.6) (0.7.5)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.10/dist-packages (from ipython>=3.2.3->pyfolio<0.10,>=0.9->finrl==0.3.6) (5.7.1)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from ipython>=3.2.3->pyfolio<0.10,>=0.9->finrl==0.3.6) (3.0.43)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.10/dist-packages (from ipython>=3.2.3->pyfolio<0.10,>=0.9->finrl==0.3.6) (2.16.1)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.10/dist-packages (from ipython>=3.2.3->pyfolio<0.10,>=0.9->finrl==0.3.6) (0.2.0)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.10/dist-packages (from ipython>=3.2.3->pyfolio<0.10,>=0.9->finrl==0.3.6) (0.1.6)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.10/dist-packages (from ipython>=3.2.3->pyfolio<0.10,>=0.9->finrl==0.3.6) (4.9.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=1.4.0->pyfolio<0.10,>=0.9->finrl==0.3.6) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=1.4.0->pyfolio<0.10,>=0.9->finrl==0.3.6) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=1.4.0->pyfolio<0.10,>=0.9->finrl==0.3.6) (4.47.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=1.4.0->pyfolio<0.10,>=0.9->finrl==0.3.6) (1.4.5)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=1.4.0->pyfolio<0.10,>=0.9->finrl==0.3.6) (3.1.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>2->alpaca-trade-api<4,>=3->finrl==0.3.6) (3.6)\n",
            "Collecting ale-py~=0.8.1 (from shimmy[atari]~=1.3.0->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6)\n",
            "  Downloading ale_py-0.8.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m39.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy>=1.2.8->jqdatasdk<2,>=1->finrl==0.3.6) (3.0.2)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (1.4.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (2.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (1.2.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (3.5.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (3.0.1)\n",
            "Collecting ply<4.0,>=3.4 (from thriftpy2>=0.3.9->jqdatasdk<2,>=1->finrl==0.3.6)\n",
            "  Downloading ply-3.11-py2.py3-none-any.whl (49 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.6/49.6 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (3.1.2)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (2.1.0)\n",
            "Collecting distlib<1,>=0.3.6 (from virtualenv<20.21.1,>=20.0.24->ray[default,tune]<3,>=2->finrl==0.3.6)\n",
            "  Downloading distlib-0.3.8-py2.py3-none-any.whl (468 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m468.9/468.9 kB\u001b[0m \u001b[31m39.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting platformdirs<4,>=2.4 (from virtualenv<20.21.1,>=20.0.24->ray[default,tune]<3,>=2->finrl==0.3.6)\n",
            "  Downloading platformdirs-3.11.0-py3-none-any.whl (17 kB)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.10/dist-packages (from gym->elegantrl@ git+https://github.com/AI4Finance-Foundation/ElegantRL.git#egg=elegantrl->finrl==0.3.6) (0.0.8)\n",
            "Collecting box2d-py==2.3.5 (from gym->elegantrl@ git+https://github.com/AI4Finance-Foundation/ElegantRL.git#egg=elegantrl->finrl==0.3.6)\n",
            "  Downloading box2d-py-2.3.5.tar.gz (374 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m374.4/374.4 kB\u001b[0m \u001b[31m48.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pygame (from stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6)\n",
            "  Downloading pygame-2.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m43.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: swig==4.* in /usr/local/lib/python3.10/dist-packages (from gym->elegantrl@ git+https://github.com/AI4Finance-Foundation/ElegantRL.git#egg=elegantrl->finrl==0.3.6) (4.1.1.post1)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema->ray[default,tune]<3,>=2->finrl==0.3.6) (2023.11.2)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema->ray[default,tune]<3,>=2->finrl==0.3.6) (0.32.0)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema->ray[default,tune]<3,>=2->finrl==0.3.6) (0.15.2)\n",
            "Collecting opencensus-context>=0.1.3 (from opencensus->ray[default,tune]<3,>=2->finrl==0.3.6)\n",
            "  Downloading opencensus_context-0.1.3-py2.py3-none-any.whl (5.1 kB)\n",
            "Requirement already satisfied: google-api-core<3.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from opencensus->ray[default,tune]<3,>=2->finrl==0.3.6) (2.11.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (3.0.0)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.10/dist-packages (from ale-py~=0.8.1->shimmy[atari]~=1.3.0->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (6.1.1)\n",
            "Requirement already satisfied: wcwidth>=0.1.4 in /usr/local/lib/python3.10/dist-packages (from blessed>=1.17.1->gpustat>=1.0.0->ray[default,tune]<3,>=2->finrl==0.3.6) (0.2.12)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography>=2.6.1->ccxt<4,>=3->finrl==0.3.6) (2.21)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core<3.0.0,>=1.0.0->opencensus->ray[default,tune]<3,>=2->finrl==0.3.6) (1.62.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.9.1->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (5.3.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.9.1->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.9.1->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard>=2.9.1->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (1.3.1)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /usr/local/lib/python3.10/dist-packages (from jedi>=0.16->ipython>=3.2.3->pyfolio<0.10,>=0.9->finrl==0.3.6) (0.8.3)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (0.1.2)\n",
            "Requirement already satisfied: qdldl in /usr/local/lib/python3.10/dist-packages (from osqp>=0.4.1->cvxpy<2.0.0,>=1.1.19->pyportfolioopt<2,>=1->finrl==0.3.6) (0.1.7.post0)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.10/dist-packages (from pexpect>4.3->ipython>=3.2.3->pyfolio<0.10,>=0.9->finrl==0.3.6) (0.7.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard>=2.9.1->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.13->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (1.3.0)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=2.9.1->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (0.5.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard>=2.9.1->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (3.2.2)\n",
            "\u001b[33mWARNING: The candidate selected for download or install is a yanked version: 'aiohttp' candidate (version 3.8.2 at https://files.pythonhosted.org/packages/8f/52/ea1e5eac3e748a94fdaafba5ab68adfb833f0cbdb68cc8149fbba5574176/aiohttp-3.8.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (from https://pypi.org/simple/aiohttp/) (requires-python:>=3.6))\n",
            "Reason for being yanked: This version includes overly restrictive multidict upper boundary disallowing multidict v6+. The previous patch version didn't have that and this is now causing dependency resolution problems for the users who have an \"incompatible\" version pinned. This is not really necessary anymore and will be addressed in the next release v3.8.3\n",
            "\n",
            "https://github.com/aio-libs/aiohttp/pull/6950\u001b[0m\u001b[33m\n",
            "\u001b[0mBuilding wheels for collected packages: finrl, pyfolio, elegantrl, empyrical, gpustat, thriftpy2, box2d-py, AutoROM.accept-rom-license\n",
            "  Building wheel for finrl (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for finrl: filename=finrl-0.3.6-py3-none-any.whl size=4670835 sha256=818c0ceadaed5255bcf7024241293a6520cd7690516ead78f635cbdb25e6179a\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-be60zcii/wheels/72/3b/1a/0fc805a8cc65ecd5bfe4f74a3c586b6075678b8ba53fd8f749\n",
            "  Building wheel for pyfolio (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyfolio: filename=pyfolio-0.9.2-py3-none-any.whl size=88650 sha256=81f06dd26c4a356d85425074c667bc49afc68ee0dd146525e9f64e67acdf2c83\n",
            "  Stored in directory: /root/.cache/pip/wheels/71/38/bc/e53700cfd8b0ad6b539d2fbaaf060ed8a299e7622a5b86ef42\n",
            "  Building wheel for elegantrl (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for elegantrl: filename=elegantrl-0.3.6-py3-none-any.whl size=197331 sha256=1948a058fb0e2b55cc462ecf3c5b25025f6b23fd0bf138de7ef072a1b15c1117\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-be60zcii/wheels/c0/51/a5/b05f165548221bc570f7223babd33e2992fa873cdcebe2d229\n",
            "  Building wheel for empyrical (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for empyrical: filename=empyrical-0.5.5-py3-none-any.whl size=39754 sha256=70e0a08720ceeb7c7ccbe5294bf0072d4d83555caff725a95b08b1769cbc4b4b\n",
            "  Stored in directory: /root/.cache/pip/wheels/0e/2e/f2/d6d2d9a1eb8fbbd9949bb5d4c00f753e3b74e5bd7ed10b1d36\n",
            "  Building wheel for gpustat (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gpustat: filename=gpustat-1.1.1-py3-none-any.whl size=26535 sha256=d4cd5c9bc7f1279a4923dcd9dba2b217dfadbc20b5d835c4fc78b5744dade271\n",
            "  Stored in directory: /root/.cache/pip/wheels/ec/d7/80/a71ba3540900e1f276bcae685efd8e590c810d2108b95f1e47\n",
            "  Building wheel for thriftpy2 (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for thriftpy2: filename=thriftpy2-0.4.17-cp310-cp310-linux_x86_64.whl size=1382608 sha256=6b2617c53755406d318995daff76fd885bdcc917955160354625d3110c2adbf7\n",
            "  Stored in directory: /root/.cache/pip/wheels/b3/ca/c1/bd8e4cd94145e4e79882fdf705ca021a7a7632248ce6d1d4f4\n",
            "  Building wheel for box2d-py (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for box2d-py: filename=box2d_py-2.3.5-cp310-cp310-linux_x86_64.whl size=2373128 sha256=f1de574e8ccb76cf8ccbe51057110a806ee1f74879fa23aade84e7fc6e06959e\n",
            "  Stored in directory: /root/.cache/pip/wheels/db/8f/6a/eaaadf056fba10a98d986f6dce954e6201ba3126926fc5ad9e\n",
            "  Building wheel for AutoROM.accept-rom-license (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for AutoROM.accept-rom-license: filename=AutoROM.accept_rom_license-0.6.1-py3-none-any.whl size=446660 sha256=eefd2bb6ed300029fa7f61a633e3821bfcbf01fa3a2daed579751e5e67442845\n",
            "  Stored in directory: /root/.cache/pip/wheels/6b/1b/ef/a43ff1a2f1736d5711faa1ba4c1f61be1131b8899e6a057811\n",
            "Successfully built finrl pyfolio elegantrl empyrical gpustat thriftpy2 box2d-py AutoROM.accept-rom-license\n",
            "Installing collected packages: py-spy, ply, opencensus-context, nvidia-ml-py, msgpack, korean-lunar-calendar, farama-notifications, distlib, colorful, box2d-py, websockets, urllib3, tzdata, thriftpy2, tensorboardX, PyYAML, pymysql, pyluach, pygame, platformdirs, multidict, jedi, gymnasium, deprecation, charset-normalizer, blessed, ale-py, virtualenv, shimmy, pycares, gpustat, stockstats, stable-baselines3, jqdatasdk, exchange-calendars, elegantrl, AutoROM.accept-rom-license, autorom, aiohttp, aiodns, ray, opencensus, empyrical, ccxt, alpaca-trade-api, aiohttp-cors, pyfolio, finrl\n",
            "  Attempting uninstall: msgpack\n",
            "    Found existing installation: msgpack 1.0.7\n",
            "    Uninstalling msgpack-1.0.7:\n",
            "      Successfully uninstalled msgpack-1.0.7\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 2.0.7\n",
            "    Uninstalling urllib3-2.0.7:\n",
            "      Successfully uninstalled urllib3-2.0.7\n",
            "  Attempting uninstall: PyYAML\n",
            "    Found existing installation: PyYAML 6.0.1\n",
            "    Uninstalling PyYAML-6.0.1:\n",
            "      Successfully uninstalled PyYAML-6.0.1\n",
            "  Attempting uninstall: pygame\n",
            "    Found existing installation: pygame 2.5.2\n",
            "    Uninstalling pygame-2.5.2:\n",
            "      Successfully uninstalled pygame-2.5.2\n",
            "  Attempting uninstall: platformdirs\n",
            "    Found existing installation: platformdirs 4.1.0\n",
            "    Uninstalling platformdirs-4.1.0:\n",
            "      Successfully uninstalled platformdirs-4.1.0\n",
            "  Attempting uninstall: multidict\n",
            "    Found existing installation: multidict 6.0.4\n",
            "    Uninstalling multidict-6.0.4:\n",
            "      Successfully uninstalled multidict-6.0.4\n",
            "  Attempting uninstall: charset-normalizer\n",
            "    Found existing installation: charset-normalizer 3.3.2\n",
            "    Uninstalling charset-normalizer-3.3.2:\n",
            "      Successfully uninstalled charset-normalizer-3.3.2\n",
            "  Attempting uninstall: aiohttp\n",
            "    Found existing installation: aiohttp 3.9.1\n",
            "    Uninstalling aiohttp-3.9.1:\n",
            "      Successfully uninstalled aiohttp-3.9.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "llmx 0.0.15a0 requires cohere, which is not installed.\n",
            "llmx 0.0.15a0 requires openai, which is not installed.\n",
            "llmx 0.0.15a0 requires tiktoken, which is not installed.\n",
            "ipython-sql 0.5.0 requires sqlalchemy>=2.0, but you have sqlalchemy 1.4.51 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed AutoROM.accept-rom-license-0.6.1 PyYAML-6.0 aiodns-3.1.1 aiohttp-3.8.2 aiohttp-cors-0.7.0 ale-py-0.8.1 alpaca-trade-api-3.0.2 autorom-0.6.1 blessed-1.20.0 box2d-py-2.3.5 ccxt-3.1.60 charset-normalizer-2.1.1 colorful-0.5.5 deprecation-2.1.0 distlib-0.3.8 elegantrl-0.3.6 empyrical-0.5.5 exchange-calendars-4.5.1 farama-notifications-0.0.4 finrl-0.3.6 gpustat-1.1.1 gymnasium-0.29.1 jedi-0.19.1 jqdatasdk-1.9.2 korean-lunar-calendar-0.3.1 msgpack-1.0.3 multidict-5.2.0 nvidia-ml-py-12.535.133 opencensus-0.11.4 opencensus-context-0.1.3 platformdirs-3.11.0 ply-3.11 py-spy-0.3.14 pycares-4.4.0 pyfolio-0.9.2 pygame-2.1.0 pyluach-2.2.0 pymysql-1.1.0 ray-2.9.0 shimmy-1.3.0 stable-baselines3-2.2.1 stockstats-0.5.4 tensorboardX-2.6.2.2 thriftpy2-0.4.17 tzdata-2023.4 urllib3-1.26.18 virtualenv-20.21.0 websockets-10.4\n"
          ]
        }
      ],
      "source": [
        "## install required packages\n",
        "!pip install swig\n",
        "!pip install wrds\n",
        "!pip install pyportfolioopt\n",
        "## install finrl library\n",
        "!pip install git+https://github.com/AI4Finance-Foundation/FinRL.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "from finrl.meta.env_stock_trading.env_stocktrading import StockTradingEnv\n",
        "from finrl.agents.stablebaselines3.models import DRLAgent\n",
        "from stable_baselines3.common.logger import configure\n",
        "from finrl import config_tickers\n",
        "from finrl.main import check_and_make_directories\n",
        "from finrl.config import INDICATORS, TRAINED_MODEL_DIR, RESULTS_DIR\n",
        "\n",
        "check_and_make_directories([TRAINED_MODEL_DIR])"
      ],
      "metadata": {
        "id": "xfQmoUq0hFxb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "313ZOiSShKbq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "80143231-f7e3-49b4-e538-de637c62bdf0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load the Data from Part 1"
      ],
      "metadata": {
        "id": "GZwClNPchbhS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train = pd.read_csv('/content/drive/My Drive/Colab Notebooks/FinRL_type/data/train_data.csv')\n",
        "\n",
        "train = train.set_index(train.columns[0])\n",
        "train.index.names = ['']"
      ],
      "metadata": {
        "id": "jd_XburYhZBh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup Environment"
      ],
      "metadata": {
        "id": "8Rwka2LsiBQ5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stock_dimension = len(train.tic.unique())\n",
        "state_space = 1 + 2*stock_dimension + len(INDICATORS)*stock_dimension\n",
        "print(f\"Stock Dimension: {stock_dimension}, State Space: {state_space}\")"
      ],
      "metadata": {
        "id": "je8iX-4QiJJY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "95675778-65c6-496e-e3d9-ac57bd87edcb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stock Dimension: 8, State Space: 81\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "buy_cost_list = sell_cost_list = [0.001] * stock_dimension\n",
        "num_stock_shares = [0] * stock_dimension\n",
        "\n",
        "env_kwargs = {\n",
        "    \"hmax\": 100,\n",
        "    \"initial_amount\": 1000000,\n",
        "    \"num_stock_shares\": num_stock_shares,\n",
        "    \"buy_cost_pct\": buy_cost_list,\n",
        "    \"sell_cost_pct\": sell_cost_list,\n",
        "    \"state_space\": state_space,\n",
        "    \"stock_dim\": stock_dimension,\n",
        "    \"tech_indicator_list\": INDICATORS,\n",
        "    \"action_space\": stock_dimension,\n",
        "    \"reward_scaling\": 1e-4\n",
        "}\n",
        "\n",
        "\n",
        "e_train_gym = StockTradingEnv(df = train, **env_kwargs)"
      ],
      "metadata": {
        "id": "UwUl-nXCiLoo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Environment for Training"
      ],
      "metadata": {
        "id": "NakislXPiSL2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "env_train, _ = e_train_gym.get_sb_env()\n",
        "print(type(env_train))"
      ],
      "metadata": {
        "id": "dUP0u55fiOXu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ec2cd042-9519-4166-80c4-3fe8066bd6eb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'stable_baselines3.common.vec_env.dummy_vec_env.DummyVecEnv'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train Agent"
      ],
      "metadata": {
        "id": "vBpWscLvDkOL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "agent = DRLAgent(env = env_train)\n",
        "\n",
        "# Set the corresponding values to 'True' for the algorithms that you want to use\n",
        "if_using_a2c = True\n",
        "if_using_ddpg = False\n",
        "if_using_ppo = True\n",
        "if_using_td3 = False\n",
        "if_using_sac = False"
      ],
      "metadata": {
        "id": "C3Dw2faxDjsH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## A2C Model\n",
        "\n",
        "The code above and below provide a healthy framework to add different agent types for training within the same notebook. For this I'll only be training an a2c model, but may add ddpg, ppo, and td3 in later iterations."
      ],
      "metadata": {
        "id": "Kv4piHG8D-__"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_a2c = agent.get_model(\"a2c\")\n",
        "model_ppo = agent.get_model('ppo')\n",
        "\n",
        "if if_using_a2c:\n",
        "  # set up logger\n",
        "  tmp_path = RESULTS_DIR + '/a2c'\n",
        "  new_logger_a2c = configure(tmp_path, [\"stdout\", \"csv\", \"tensorboard\"])\n",
        "  # Set new logger\n",
        "  model_a2c.set_logger(new_logger_a2c)\n",
        "\n",
        "if if_using_ppo:\n",
        "  # set up logger\n",
        "  tmp_path = RESULTS_DIR + '/ppo'\n",
        "  new_logger_ppo = configure(tmp_path, [\"stdout\", \"csv\", \"tensorboard\"])\n",
        "  # Set new logger\n",
        "  model_ppo.set_logger(new_logger_ppo)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WrhYo_0ADwL9",
        "outputId": "0911c8f4-406c-48d1-cc9d-fec816c3b182"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'n_steps': 5, 'ent_coef': 0.01, 'learning_rate': 0.0007}\n",
            "Using cuda device\n",
            "{'n_steps': 2048, 'ent_coef': 0.01, 'learning_rate': 0.00025, 'batch_size': 64}\n",
            "Using cuda device\n",
            "Logging to results/a2c\n",
            "Logging to results/ppo\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trained_a2c = agent.train_model(model=model_a2c,\n",
        "                             tb_log_name='a2c',\n",
        "                             total_timesteps=50000) if if_using_a2c else None\n",
        "\n",
        "trained_ppo = agent.train_model(model=model_ppo,\n",
        "                             tb_log_name='ppo',\n",
        "                             total_timesteps=50000) if if_using_ppo else None"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7CT6vhWZD4K7",
        "outputId": "0fb2a5a2-cd4e-4031-aac6-5ae12d28e7aa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 129         |\n",
            "|    iterations         | 100         |\n",
            "|    time_elapsed       | 3           |\n",
            "|    total_timesteps    | 500         |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -11.5       |\n",
            "|    explained_variance | 0           |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 99          |\n",
            "|    policy_loss        | 23.9        |\n",
            "|    reward             | -0.70704275 |\n",
            "|    std                | 1.02        |\n",
            "|    value_loss         | 3.55        |\n",
            "---------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 161       |\n",
            "|    iterations         | 200       |\n",
            "|    time_elapsed       | 6         |\n",
            "|    total_timesteps    | 1000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.5     |\n",
            "|    explained_variance | 0.0222    |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 199       |\n",
            "|    policy_loss        | -25.3     |\n",
            "|    reward             | -3.206995 |\n",
            "|    std                | 1.02      |\n",
            "|    value_loss         | 8.31      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 176       |\n",
            "|    iterations         | 300       |\n",
            "|    time_elapsed       | 8         |\n",
            "|    total_timesteps    | 1500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.4     |\n",
            "|    explained_variance | -0.336    |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 299       |\n",
            "|    policy_loss        | -57.5     |\n",
            "|    reward             | 6.4251018 |\n",
            "|    std                | 1.01      |\n",
            "|    value_loss         | 32        |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 184       |\n",
            "|    iterations         | 400       |\n",
            "|    time_elapsed       | 10        |\n",
            "|    total_timesteps    | 2000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.4     |\n",
            "|    explained_variance | -0.0772   |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 399       |\n",
            "|    policy_loss        | -8.43     |\n",
            "|    reward             | 3.7559714 |\n",
            "|    std                | 1.01      |\n",
            "|    value_loss         | 8.71      |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 181        |\n",
            "|    iterations         | 500        |\n",
            "|    time_elapsed       | 13         |\n",
            "|    total_timesteps    | 2500       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.5      |\n",
            "|    explained_variance | 0.00279    |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 499        |\n",
            "|    policy_loss        | 172        |\n",
            "|    reward             | -20.466377 |\n",
            "|    std                | 1.01       |\n",
            "|    value_loss         | 479        |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 175       |\n",
            "|    iterations         | 600       |\n",
            "|    time_elapsed       | 17        |\n",
            "|    total_timesteps    | 3000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.5     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 599       |\n",
            "|    policy_loss        | 29.2      |\n",
            "|    reward             | 0.2522563 |\n",
            "|    std                | 1.02      |\n",
            "|    value_loss         | 5.78      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 180       |\n",
            "|    iterations         | 700       |\n",
            "|    time_elapsed       | 19        |\n",
            "|    total_timesteps    | 3500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.5     |\n",
            "|    explained_variance | 0.314     |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 699       |\n",
            "|    policy_loss        | -38.2     |\n",
            "|    reward             | -5.573826 |\n",
            "|    std                | 1.02      |\n",
            "|    value_loss         | 11.7      |\n",
            "-------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 184         |\n",
            "|    iterations         | 800         |\n",
            "|    time_elapsed       | 21          |\n",
            "|    total_timesteps    | 4000        |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -11.5       |\n",
            "|    explained_variance | 0.1         |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 799         |\n",
            "|    policy_loss        | 37.6        |\n",
            "|    reward             | -0.39642093 |\n",
            "|    std                | 1.02        |\n",
            "|    value_loss         | 7.48        |\n",
            "---------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 187        |\n",
            "|    iterations         | 900        |\n",
            "|    time_elapsed       | 24         |\n",
            "|    total_timesteps    | 4500       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.5      |\n",
            "|    explained_variance | -0.015     |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 899        |\n",
            "|    policy_loss        | 51.4       |\n",
            "|    reward             | -0.6331869 |\n",
            "|    std                | 1.02       |\n",
            "|    value_loss         | 25.5       |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 189        |\n",
            "|    iterations         | 1000       |\n",
            "|    time_elapsed       | 26         |\n",
            "|    total_timesteps    | 5000       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.5      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 999        |\n",
            "|    policy_loss        | 35         |\n",
            "|    reward             | -12.692615 |\n",
            "|    std                | 1.02       |\n",
            "|    value_loss         | 15.4       |\n",
            "--------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 184      |\n",
            "|    iterations         | 1100     |\n",
            "|    time_elapsed       | 29       |\n",
            "|    total_timesteps    | 5500     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -11.5    |\n",
            "|    explained_variance | 0.01     |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 1099     |\n",
            "|    policy_loss        | -239     |\n",
            "|    reward             | 8.791125 |\n",
            "|    std                | 1.02     |\n",
            "|    value_loss         | 461      |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 183       |\n",
            "|    iterations         | 1200      |\n",
            "|    time_elapsed       | 32        |\n",
            "|    total_timesteps    | 6000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.5     |\n",
            "|    explained_variance | -0.0432   |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 1199      |\n",
            "|    policy_loss        | -23.7     |\n",
            "|    reward             | 0.5310104 |\n",
            "|    std                | 1.02      |\n",
            "|    value_loss         | 7.87      |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 185      |\n",
            "|    iterations         | 1300     |\n",
            "|    time_elapsed       | 34       |\n",
            "|    total_timesteps    | 6500     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -11.5    |\n",
            "|    explained_variance | 0.0816   |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 1299     |\n",
            "|    policy_loss        | 29.6     |\n",
            "|    reward             | -4.62948 |\n",
            "|    std                | 1.02     |\n",
            "|    value_loss         | 21.1     |\n",
            "------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 187         |\n",
            "|    iterations         | 1400        |\n",
            "|    time_elapsed       | 37          |\n",
            "|    total_timesteps    | 7000        |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -11.5       |\n",
            "|    explained_variance | 0.00179     |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 1399        |\n",
            "|    policy_loss        | 35.8        |\n",
            "|    reward             | -0.08347812 |\n",
            "|    std                | 1.02        |\n",
            "|    value_loss         | 13.6        |\n",
            "---------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 189      |\n",
            "|    iterations         | 1500     |\n",
            "|    time_elapsed       | 39       |\n",
            "|    total_timesteps    | 7500     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -11.5    |\n",
            "|    explained_variance | -0.0979  |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 1499     |\n",
            "|    policy_loss        | 15.5     |\n",
            "|    reward             | 9.730231 |\n",
            "|    std                | 1.02     |\n",
            "|    value_loss         | 3.97     |\n",
            "------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 188         |\n",
            "|    iterations         | 1600        |\n",
            "|    time_elapsed       | 42          |\n",
            "|    total_timesteps    | 8000        |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -11.6       |\n",
            "|    explained_variance | 0.059       |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 1599        |\n",
            "|    policy_loss        | 16.8        |\n",
            "|    reward             | -0.18778202 |\n",
            "|    std                | 1.03        |\n",
            "|    value_loss         | 5.91        |\n",
            "---------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 184       |\n",
            "|    iterations         | 1700      |\n",
            "|    time_elapsed       | 45        |\n",
            "|    total_timesteps    | 8500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.5     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 1699      |\n",
            "|    policy_loss        | -47.2     |\n",
            "|    reward             | 5.8774996 |\n",
            "|    std                | 1.02      |\n",
            "|    value_loss         | 63.6      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 186       |\n",
            "|    iterations         | 1800      |\n",
            "|    time_elapsed       | 48        |\n",
            "|    total_timesteps    | 9000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.5     |\n",
            "|    explained_variance | -0.0169   |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 1799      |\n",
            "|    policy_loss        | 1.44      |\n",
            "|    reward             | 2.6111124 |\n",
            "|    std                | 1.02      |\n",
            "|    value_loss         | 0.778     |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 187       |\n",
            "|    iterations         | 1900      |\n",
            "|    time_elapsed       | 50        |\n",
            "|    total_timesteps    | 9500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.5     |\n",
            "|    explained_variance | 0.00259   |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 1899      |\n",
            "|    policy_loss        | -14.4     |\n",
            "|    reward             | 2.2003891 |\n",
            "|    std                | 1.02      |\n",
            "|    value_loss         | 3.81      |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 188        |\n",
            "|    iterations         | 2000       |\n",
            "|    time_elapsed       | 53         |\n",
            "|    total_timesteps    | 10000      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.6      |\n",
            "|    explained_variance | 0.015      |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 1999       |\n",
            "|    policy_loss        | -30.2      |\n",
            "|    reward             | -1.8211728 |\n",
            "|    std                | 1.03       |\n",
            "|    value_loss         | 8.28       |\n",
            "--------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 189      |\n",
            "|    iterations         | 2100     |\n",
            "|    time_elapsed       | 55       |\n",
            "|    total_timesteps    | 10500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -11.6    |\n",
            "|    explained_variance | 0.00238  |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 2099     |\n",
            "|    policy_loss        | 14       |\n",
            "|    reward             | 4.084419 |\n",
            "|    std                | 1.03     |\n",
            "|    value_loss         | 8.75     |\n",
            "------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 187        |\n",
            "|    iterations         | 2200       |\n",
            "|    time_elapsed       | 58         |\n",
            "|    total_timesteps    | 11000      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.6      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 2199       |\n",
            "|    policy_loss        | -20.9      |\n",
            "|    reward             | -15.921975 |\n",
            "|    std                | 1.03       |\n",
            "|    value_loss         | 54.3       |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 186       |\n",
            "|    iterations         | 2300      |\n",
            "|    time_elapsed       | 61        |\n",
            "|    total_timesteps    | 11500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.6     |\n",
            "|    explained_variance | -0.000158 |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 2299      |\n",
            "|    policy_loss        | -1.43e+03 |\n",
            "|    reward             | 13.86564  |\n",
            "|    std                | 1.03      |\n",
            "|    value_loss         | 1.92e+04  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 187       |\n",
            "|    iterations         | 2400      |\n",
            "|    time_elapsed       | 63        |\n",
            "|    total_timesteps    | 12000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.6     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 2399      |\n",
            "|    policy_loss        | 37.9      |\n",
            "|    reward             | 0.4219774 |\n",
            "|    std                | 1.03      |\n",
            "|    value_loss         | 21.8      |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 188      |\n",
            "|    iterations         | 2500     |\n",
            "|    time_elapsed       | 66       |\n",
            "|    total_timesteps    | 12500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -11.6    |\n",
            "|    explained_variance | 0.237    |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 2499     |\n",
            "|    policy_loss        | -49.7    |\n",
            "|    reward             | 3.065181 |\n",
            "|    std                | 1.03     |\n",
            "|    value_loss         | 17.2     |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 189       |\n",
            "|    iterations         | 2600      |\n",
            "|    time_elapsed       | 68        |\n",
            "|    total_timesteps    | 13000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.7     |\n",
            "|    explained_variance | 0.0753    |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 2599      |\n",
            "|    policy_loss        | 21.1      |\n",
            "|    reward             | 2.6336682 |\n",
            "|    std                | 1.04      |\n",
            "|    value_loss         | 3.75      |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 188        |\n",
            "|    iterations         | 2700       |\n",
            "|    time_elapsed       | 71         |\n",
            "|    total_timesteps    | 13500      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.7      |\n",
            "|    explained_variance | -0.0638    |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 2699       |\n",
            "|    policy_loss        | 9.11       |\n",
            "|    reward             | -1.5121511 |\n",
            "|    std                | 1.04       |\n",
            "|    value_loss         | 1.74       |\n",
            "--------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 186      |\n",
            "|    iterations         | 2800     |\n",
            "|    time_elapsed       | 74       |\n",
            "|    total_timesteps    | 14000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -11.7    |\n",
            "|    explained_variance | -0.047   |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 2799     |\n",
            "|    policy_loss        | 149      |\n",
            "|    reward             | 3.92476  |\n",
            "|    std                | 1.04     |\n",
            "|    value_loss         | 194      |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 187       |\n",
            "|    iterations         | 2900      |\n",
            "|    time_elapsed       | 77        |\n",
            "|    total_timesteps    | 14500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.7     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 2899      |\n",
            "|    policy_loss        | -12.7     |\n",
            "|    reward             | 0.6924594 |\n",
            "|    std                | 1.04      |\n",
            "|    value_loss         | 1.15      |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 188      |\n",
            "|    iterations         | 3000     |\n",
            "|    time_elapsed       | 79       |\n",
            "|    total_timesteps    | 15000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -11.7    |\n",
            "|    explained_variance | -0.0946  |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 2999     |\n",
            "|    policy_loss        | 7.91     |\n",
            "|    reward             | 1.153079 |\n",
            "|    std                | 1.04     |\n",
            "|    value_loss         | 1.51     |\n",
            "------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 188         |\n",
            "|    iterations         | 3100        |\n",
            "|    time_elapsed       | 82          |\n",
            "|    total_timesteps    | 15500       |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -11.7       |\n",
            "|    explained_variance | -0.0325     |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 3099        |\n",
            "|    policy_loss        | -25         |\n",
            "|    reward             | -0.33527043 |\n",
            "|    std                | 1.04        |\n",
            "|    value_loss         | 4.83        |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 189         |\n",
            "|    iterations         | 3200        |\n",
            "|    time_elapsed       | 84          |\n",
            "|    total_timesteps    | 16000       |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -11.7       |\n",
            "|    explained_variance | -0.00521    |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 3199        |\n",
            "|    policy_loss        | 13.2        |\n",
            "|    reward             | -0.93547666 |\n",
            "|    std                | 1.04        |\n",
            "|    value_loss         | 10.4        |\n",
            "---------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 188        |\n",
            "|    iterations         | 3300       |\n",
            "|    time_elapsed       | 87         |\n",
            "|    total_timesteps    | 16500      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.7      |\n",
            "|    explained_variance | 0.0271     |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 3299       |\n",
            "|    policy_loss        | 109        |\n",
            "|    reward             | 0.26852724 |\n",
            "|    std                | 1.04       |\n",
            "|    value_loss         | 124        |\n",
            "--------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 187      |\n",
            "|    iterations         | 3400     |\n",
            "|    time_elapsed       | 90       |\n",
            "|    total_timesteps    | 17000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -11.7    |\n",
            "|    explained_variance | -0.221   |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 3399     |\n",
            "|    policy_loss        | 153      |\n",
            "|    reward             | 6.320787 |\n",
            "|    std                | 1.05     |\n",
            "|    value_loss         | 201      |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 188       |\n",
            "|    iterations         | 3500      |\n",
            "|    time_elapsed       | 93        |\n",
            "|    total_timesteps    | 17500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.7     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 3499      |\n",
            "|    policy_loss        | 70.7      |\n",
            "|    reward             | 0.6196914 |\n",
            "|    std                | 1.05      |\n",
            "|    value_loss         | 33.1      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 188       |\n",
            "|    iterations         | 3600      |\n",
            "|    time_elapsed       | 95        |\n",
            "|    total_timesteps    | 18000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.7     |\n",
            "|    explained_variance | 0.000581  |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 3599      |\n",
            "|    policy_loss        | -39.3     |\n",
            "|    reward             | 1.6604995 |\n",
            "|    std                | 1.05      |\n",
            "|    value_loss         | 18        |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 189        |\n",
            "|    iterations         | 3700       |\n",
            "|    time_elapsed       | 97         |\n",
            "|    total_timesteps    | 18500      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.7      |\n",
            "|    explained_variance | -0.00493   |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 3699       |\n",
            "|    policy_loss        | 58.4       |\n",
            "|    reward             | -1.3176129 |\n",
            "|    std                | 1.05       |\n",
            "|    value_loss         | 28.1       |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 189       |\n",
            "|    iterations         | 3800      |\n",
            "|    time_elapsed       | 100       |\n",
            "|    total_timesteps    | 19000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.8     |\n",
            "|    explained_variance | -0.0168   |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 3799      |\n",
            "|    policy_loss        | -8.49     |\n",
            "|    reward             | 1.0138335 |\n",
            "|    std                | 1.06      |\n",
            "|    value_loss         | 1.92      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 187       |\n",
            "|    iterations         | 3900      |\n",
            "|    time_elapsed       | 104       |\n",
            "|    total_timesteps    | 19500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.8     |\n",
            "|    explained_variance | 0.00874   |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 3899      |\n",
            "|    policy_loss        | -40.8     |\n",
            "|    reward             | 3.1033936 |\n",
            "|    std                | 1.05      |\n",
            "|    value_loss         | 18.8      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 188       |\n",
            "|    iterations         | 4000      |\n",
            "|    time_elapsed       | 106       |\n",
            "|    total_timesteps    | 20000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.8     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 3999      |\n",
            "|    policy_loss        | 48        |\n",
            "|    reward             | 5.1504164 |\n",
            "|    std                | 1.05      |\n",
            "|    value_loss         | 27        |\n",
            "-------------------------------------\n",
            "----------------------------------------\n",
            "| time/                 |              |\n",
            "|    fps                | 188          |\n",
            "|    iterations         | 4100         |\n",
            "|    time_elapsed       | 108          |\n",
            "|    total_timesteps    | 20500        |\n",
            "| train/                |              |\n",
            "|    entropy_loss       | -11.8        |\n",
            "|    explained_variance | 0.00835      |\n",
            "|    learning_rate      | 0.0007       |\n",
            "|    n_updates          | 4099         |\n",
            "|    policy_loss        | -5.42        |\n",
            "|    reward             | -0.048128206 |\n",
            "|    std                | 1.06         |\n",
            "|    value_loss         | 1.15         |\n",
            "----------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 189         |\n",
            "|    iterations         | 4200        |\n",
            "|    time_elapsed       | 111         |\n",
            "|    total_timesteps    | 21000       |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -11.8       |\n",
            "|    explained_variance | -0.0051     |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 4199        |\n",
            "|    policy_loss        | -132        |\n",
            "|    reward             | -0.37871155 |\n",
            "|    std                | 1.06        |\n",
            "|    value_loss         | 161         |\n",
            "---------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 189       |\n",
            "|    iterations         | 4300      |\n",
            "|    time_elapsed       | 113       |\n",
            "|    total_timesteps    | 21500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.8     |\n",
            "|    explained_variance | 0.0843    |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 4299      |\n",
            "|    policy_loss        | -15.5     |\n",
            "|    reward             | 2.4989233 |\n",
            "|    std                | 1.06      |\n",
            "|    value_loss         | 4.5       |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 188       |\n",
            "|    iterations         | 4400      |\n",
            "|    time_elapsed       | 116       |\n",
            "|    total_timesteps    | 22000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.8     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 4399      |\n",
            "|    policy_loss        | -60.6     |\n",
            "|    reward             | 1.2000178 |\n",
            "|    std                | 1.06      |\n",
            "|    value_loss         | 42.6      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 187       |\n",
            "|    iterations         | 4500      |\n",
            "|    time_elapsed       | 119       |\n",
            "|    total_timesteps    | 22500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.8     |\n",
            "|    explained_variance | 0.00506   |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 4499      |\n",
            "|    policy_loss        | 94.7      |\n",
            "|    reward             | 6.5329285 |\n",
            "|    std                | 1.06      |\n",
            "|    value_loss         | 54.2      |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 188        |\n",
            "|    iterations         | 4600       |\n",
            "|    time_elapsed       | 122        |\n",
            "|    total_timesteps    | 23000      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.8      |\n",
            "|    explained_variance | 0.0175     |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 4599       |\n",
            "|    policy_loss        | 70.6       |\n",
            "|    reward             | 0.93505937 |\n",
            "|    std                | 1.06       |\n",
            "|    value_loss         | 68         |\n",
            "--------------------------------------\n",
            "----------------------------------------\n",
            "| time/                 |              |\n",
            "|    fps                | 188          |\n",
            "|    iterations         | 4700         |\n",
            "|    time_elapsed       | 124          |\n",
            "|    total_timesteps    | 23500        |\n",
            "| train/                |              |\n",
            "|    entropy_loss       | -11.8        |\n",
            "|    explained_variance | -0.000449    |\n",
            "|    learning_rate      | 0.0007       |\n",
            "|    n_updates          | 4699         |\n",
            "|    policy_loss        | -23.5        |\n",
            "|    reward             | 0.0017710311 |\n",
            "|    std                | 1.06         |\n",
            "|    value_loss         | 9.52         |\n",
            "----------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 189       |\n",
            "|    iterations         | 4800      |\n",
            "|    time_elapsed       | 126       |\n",
            "|    total_timesteps    | 24000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.7     |\n",
            "|    explained_variance | 0.0168    |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 4799      |\n",
            "|    policy_loss        | -99.4     |\n",
            "|    reward             | -2.443155 |\n",
            "|    std                | 1.05      |\n",
            "|    value_loss         | 111       |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 189       |\n",
            "|    iterations         | 4900      |\n",
            "|    time_elapsed       | 129       |\n",
            "|    total_timesteps    | 24500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.8     |\n",
            "|    explained_variance | -0.156    |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 4899      |\n",
            "|    policy_loss        | -18.7     |\n",
            "|    reward             | 2.3197877 |\n",
            "|    std                | 1.05      |\n",
            "|    value_loss         | 6.32      |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 188        |\n",
            "|    iterations         | 5000       |\n",
            "|    time_elapsed       | 132        |\n",
            "|    total_timesteps    | 25000      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.7      |\n",
            "|    explained_variance | 0.0164     |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 4999       |\n",
            "|    policy_loss        | -28.2      |\n",
            "|    reward             | -0.9628963 |\n",
            "|    std                | 1.05       |\n",
            "|    value_loss         | 9.1        |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 188       |\n",
            "|    iterations         | 5100      |\n",
            "|    time_elapsed       | 135       |\n",
            "|    total_timesteps    | 25500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.7     |\n",
            "|    explained_variance | 0.143     |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 5099      |\n",
            "|    policy_loss        | 103       |\n",
            "|    reward             | 1.5253209 |\n",
            "|    std                | 1.05      |\n",
            "|    value_loss         | 77.9      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 188       |\n",
            "|    iterations         | 5200      |\n",
            "|    time_elapsed       | 137       |\n",
            "|    total_timesteps    | 26000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.7     |\n",
            "|    explained_variance | 0.0623    |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 5199      |\n",
            "|    policy_loss        | -118      |\n",
            "|    reward             | 12.360501 |\n",
            "|    std                | 1.05      |\n",
            "|    value_loss         | 172       |\n",
            "-------------------------------------\n",
            "day: 2892, episode: 10\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 5367787.09\n",
            "total_reward: 4367787.09\n",
            "total_cost: 65130.23\n",
            "total_trades: 19613\n",
            "Sharpe: 0.765\n",
            "=================================\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 189         |\n",
            "|    iterations         | 5300        |\n",
            "|    time_elapsed       | 140         |\n",
            "|    total_timesteps    | 26500       |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -11.8       |\n",
            "|    explained_variance | 0.00536     |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 5299        |\n",
            "|    policy_loss        | -15.2       |\n",
            "|    reward             | 0.099125564 |\n",
            "|    std                | 1.05        |\n",
            "|    value_loss         | 1.71        |\n",
            "---------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 189        |\n",
            "|    iterations         | 5400       |\n",
            "|    time_elapsed       | 142        |\n",
            "|    total_timesteps    | 27000      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.8      |\n",
            "|    explained_variance | 0.00725    |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 5399       |\n",
            "|    policy_loss        | -49        |\n",
            "|    reward             | 0.17543544 |\n",
            "|    std                | 1.06       |\n",
            "|    value_loss         | 25.8       |\n",
            "--------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 188      |\n",
            "|    iterations         | 5500     |\n",
            "|    time_elapsed       | 145      |\n",
            "|    total_timesteps    | 27500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -11.8    |\n",
            "|    explained_variance | -0.36    |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 5499     |\n",
            "|    policy_loss        | 31.2     |\n",
            "|    reward             | 1.289808 |\n",
            "|    std                | 1.06     |\n",
            "|    value_loss         | 15.7     |\n",
            "------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 188        |\n",
            "|    iterations         | 5600       |\n",
            "|    time_elapsed       | 148        |\n",
            "|    total_timesteps    | 28000      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.8      |\n",
            "|    explained_variance | -0.208     |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 5599       |\n",
            "|    policy_loss        | -39        |\n",
            "|    reward             | -1.9884222 |\n",
            "|    std                | 1.07       |\n",
            "|    value_loss         | 17.9       |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 188       |\n",
            "|    iterations         | 5700      |\n",
            "|    time_elapsed       | 151       |\n",
            "|    total_timesteps    | 28500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.8     |\n",
            "|    explained_variance | -0.0228   |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 5699      |\n",
            "|    policy_loss        | -116      |\n",
            "|    reward             | -4.368585 |\n",
            "|    std                | 1.06      |\n",
            "|    value_loss         | 149       |\n",
            "-------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 189         |\n",
            "|    iterations         | 5800        |\n",
            "|    time_elapsed       | 153         |\n",
            "|    total_timesteps    | 29000       |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -11.8       |\n",
            "|    explained_variance | 1.19e-07    |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 5799        |\n",
            "|    policy_loss        | -2.16       |\n",
            "|    reward             | 0.060237486 |\n",
            "|    std                | 1.06        |\n",
            "|    value_loss         | 0.681       |\n",
            "---------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 189       |\n",
            "|    iterations         | 5900      |\n",
            "|    time_elapsed       | 155       |\n",
            "|    total_timesteps    | 29500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.8     |\n",
            "|    explained_variance | 0.00011   |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 5899      |\n",
            "|    policy_loss        | 19.2      |\n",
            "|    reward             | -1.953739 |\n",
            "|    std                | 1.06      |\n",
            "|    value_loss         | 5.8       |\n",
            "-------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 189         |\n",
            "|    iterations         | 6000        |\n",
            "|    time_elapsed       | 158         |\n",
            "|    total_timesteps    | 30000       |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -11.8       |\n",
            "|    explained_variance | -3.3        |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 5999        |\n",
            "|    policy_loss        | -7.57       |\n",
            "|    reward             | -0.33556548 |\n",
            "|    std                | 1.06        |\n",
            "|    value_loss         | 2.47        |\n",
            "---------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 188        |\n",
            "|    iterations         | 6100       |\n",
            "|    time_elapsed       | 161        |\n",
            "|    total_timesteps    | 30500      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.9      |\n",
            "|    explained_variance | 0.0979     |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 6099       |\n",
            "|    policy_loss        | -11.2      |\n",
            "|    reward             | -3.0115745 |\n",
            "|    std                | 1.07       |\n",
            "|    value_loss         | 6.14       |\n",
            "--------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 188         |\n",
            "|    iterations         | 6200        |\n",
            "|    time_elapsed       | 164         |\n",
            "|    total_timesteps    | 31000       |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -11.9       |\n",
            "|    explained_variance | 0.04        |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 6199        |\n",
            "|    policy_loss        | -1.21       |\n",
            "|    reward             | -0.36397842 |\n",
            "|    std                | 1.07        |\n",
            "|    value_loss         | 1.09        |\n",
            "---------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 189      |\n",
            "|    iterations         | 6300     |\n",
            "|    time_elapsed       | 166      |\n",
            "|    total_timesteps    | 31500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -11.8    |\n",
            "|    explained_variance | -0.0956  |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 6299     |\n",
            "|    policy_loss        | 26.3     |\n",
            "|    reward             | 3.879102 |\n",
            "|    std                | 1.06     |\n",
            "|    value_loss         | 11.1     |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 189       |\n",
            "|    iterations         | 6400      |\n",
            "|    time_elapsed       | 168       |\n",
            "|    total_timesteps    | 32000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.8     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 6399      |\n",
            "|    policy_loss        | 33        |\n",
            "|    reward             | 2.0087361 |\n",
            "|    std                | 1.06      |\n",
            "|    value_loss         | 9.66      |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 189        |\n",
            "|    iterations         | 6500       |\n",
            "|    time_elapsed       | 171        |\n",
            "|    total_timesteps    | 32500      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.8      |\n",
            "|    explained_variance | 0.0388     |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 6499       |\n",
            "|    policy_loss        | 12.3       |\n",
            "|    reward             | -4.9899054 |\n",
            "|    std                | 1.06       |\n",
            "|    value_loss         | 15.7       |\n",
            "--------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 189         |\n",
            "|    iterations         | 6600        |\n",
            "|    time_elapsed       | 174         |\n",
            "|    total_timesteps    | 33000       |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -11.9       |\n",
            "|    explained_variance | 0.203       |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 6599        |\n",
            "|    policy_loss        | -7.38       |\n",
            "|    reward             | -0.18015185 |\n",
            "|    std                | 1.07        |\n",
            "|    value_loss         | 1.3         |\n",
            "---------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 188       |\n",
            "|    iterations         | 6700      |\n",
            "|    time_elapsed       | 177       |\n",
            "|    total_timesteps    | 33500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.8     |\n",
            "|    explained_variance | 0.0154    |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 6699      |\n",
            "|    policy_loss        | -244      |\n",
            "|    reward             | -8.375147 |\n",
            "|    std                | 1.06      |\n",
            "|    value_loss         | 499       |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 188       |\n",
            "|    iterations         | 6800      |\n",
            "|    time_elapsed       | 179       |\n",
            "|    total_timesteps    | 34000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.8     |\n",
            "|    explained_variance | -0.341    |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 6799      |\n",
            "|    policy_loss        | -74.4     |\n",
            "|    reward             | 1.4039842 |\n",
            "|    std                | 1.06      |\n",
            "|    value_loss         | 43.4      |\n",
            "-------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 189         |\n",
            "|    iterations         | 6900        |\n",
            "|    time_elapsed       | 182         |\n",
            "|    total_timesteps    | 34500       |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -11.8       |\n",
            "|    explained_variance | -0.011      |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 6899        |\n",
            "|    policy_loss        | -107        |\n",
            "|    reward             | -0.34134388 |\n",
            "|    std                | 1.06        |\n",
            "|    value_loss         | 143         |\n",
            "---------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 189        |\n",
            "|    iterations         | 7000       |\n",
            "|    time_elapsed       | 184        |\n",
            "|    total_timesteps    | 35000      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.8      |\n",
            "|    explained_variance | -2.53      |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 6999       |\n",
            "|    policy_loss        | 10.7       |\n",
            "|    reward             | -0.6051036 |\n",
            "|    std                | 1.07       |\n",
            "|    value_loss         | 1.4        |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 189        |\n",
            "|    iterations         | 7100       |\n",
            "|    time_elapsed       | 186        |\n",
            "|    total_timesteps    | 35500      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.9      |\n",
            "|    explained_variance | -1.39      |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 7099       |\n",
            "|    policy_loss        | 0.531      |\n",
            "|    reward             | 0.26066968 |\n",
            "|    std                | 1.07       |\n",
            "|    value_loss         | 1.03       |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 188        |\n",
            "|    iterations         | 7200       |\n",
            "|    time_elapsed       | 190        |\n",
            "|    total_timesteps    | 36000      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.9      |\n",
            "|    explained_variance | -0.215     |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 7199       |\n",
            "|    policy_loss        | -56        |\n",
            "|    reward             | 0.13294648 |\n",
            "|    std                | 1.07       |\n",
            "|    value_loss         | 23.8       |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 188       |\n",
            "|    iterations         | 7300      |\n",
            "|    time_elapsed       | 193       |\n",
            "|    total_timesteps    | 36500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.9     |\n",
            "|    explained_variance | -0.151    |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 7299      |\n",
            "|    policy_loss        | -2.74     |\n",
            "|    reward             | 2.5168767 |\n",
            "|    std                | 1.08      |\n",
            "|    value_loss         | 1.34      |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 189        |\n",
            "|    iterations         | 7400       |\n",
            "|    time_elapsed       | 195        |\n",
            "|    total_timesteps    | 37000      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.9      |\n",
            "|    explained_variance | 0.0246     |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 7399       |\n",
            "|    policy_loss        | 34.1       |\n",
            "|    reward             | -4.6254025 |\n",
            "|    std                | 1.08       |\n",
            "|    value_loss         | 15.3       |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 189        |\n",
            "|    iterations         | 7500       |\n",
            "|    time_elapsed       | 197        |\n",
            "|    total_timesteps    | 37500      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.9      |\n",
            "|    explained_variance | -0.166     |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 7499       |\n",
            "|    policy_loss        | 78.3       |\n",
            "|    reward             | -5.8522525 |\n",
            "|    std                | 1.08       |\n",
            "|    value_loss         | 46.1       |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 189        |\n",
            "|    iterations         | 7600       |\n",
            "|    time_elapsed       | 200        |\n",
            "|    total_timesteps    | 38000      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -12        |\n",
            "|    explained_variance | 0.204      |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 7599       |\n",
            "|    policy_loss        | -45.9      |\n",
            "|    reward             | 0.62584805 |\n",
            "|    std                | 1.08       |\n",
            "|    value_loss         | 15.8       |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 189       |\n",
            "|    iterations         | 7700      |\n",
            "|    time_elapsed       | 203       |\n",
            "|    total_timesteps    | 38500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.9     |\n",
            "|    explained_variance | -0.215    |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 7699      |\n",
            "|    policy_loss        | -89       |\n",
            "|    reward             | -0.893176 |\n",
            "|    std                | 1.08      |\n",
            "|    value_loss         | 77.9      |\n",
            "-------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 188         |\n",
            "|    iterations         | 7800        |\n",
            "|    time_elapsed       | 206         |\n",
            "|    total_timesteps    | 39000       |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -12         |\n",
            "|    explained_variance | -0.0723     |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 7799        |\n",
            "|    policy_loss        | -5.45       |\n",
            "|    reward             | -0.74029624 |\n",
            "|    std                | 1.08        |\n",
            "|    value_loss         | 5.37        |\n",
            "---------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 188       |\n",
            "|    iterations         | 7900      |\n",
            "|    time_elapsed       | 209       |\n",
            "|    total_timesteps    | 39500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12       |\n",
            "|    explained_variance | 0.0262    |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 7899      |\n",
            "|    policy_loss        | 58.9      |\n",
            "|    reward             | 3.5103567 |\n",
            "|    std                | 1.08      |\n",
            "|    value_loss         | 45.7      |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 189        |\n",
            "|    iterations         | 8000       |\n",
            "|    time_elapsed       | 211        |\n",
            "|    total_timesteps    | 40000      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -12        |\n",
            "|    explained_variance | -0.149     |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 7999       |\n",
            "|    policy_loss        | -92.4      |\n",
            "|    reward             | -1.6066399 |\n",
            "|    std                | 1.08       |\n",
            "|    value_loss         | 64.4       |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 189       |\n",
            "|    iterations         | 8100      |\n",
            "|    time_elapsed       | 213       |\n",
            "|    total_timesteps    | 40500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12       |\n",
            "|    explained_variance | -0.0271   |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 8099      |\n",
            "|    policy_loss        | -47.6     |\n",
            "|    reward             | 4.5945344 |\n",
            "|    std                | 1.08      |\n",
            "|    value_loss         | 18.5      |\n",
            "-------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 189         |\n",
            "|    iterations         | 8200        |\n",
            "|    time_elapsed       | 216         |\n",
            "|    total_timesteps    | 41000       |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -12         |\n",
            "|    explained_variance | -1.38       |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 8199        |\n",
            "|    policy_loss        | 10.4        |\n",
            "|    reward             | -0.09127973 |\n",
            "|    std                | 1.08        |\n",
            "|    value_loss         | 1.27        |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 188         |\n",
            "|    iterations         | 8300        |\n",
            "|    time_elapsed       | 219         |\n",
            "|    total_timesteps    | 41500       |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -11.9       |\n",
            "|    explained_variance | -0.233      |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 8299        |\n",
            "|    policy_loss        | 17.9        |\n",
            "|    reward             | -0.22203425 |\n",
            "|    std                | 1.08        |\n",
            "|    value_loss         | 5.03        |\n",
            "---------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 188        |\n",
            "|    iterations         | 8400       |\n",
            "|    time_elapsed       | 222        |\n",
            "|    total_timesteps    | 42000      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -12        |\n",
            "|    explained_variance | -0.351     |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 8399       |\n",
            "|    policy_loss        | -21.3      |\n",
            "|    reward             | -2.5207045 |\n",
            "|    std                | 1.08       |\n",
            "|    value_loss         | 4.83       |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 189        |\n",
            "|    iterations         | 8500       |\n",
            "|    time_elapsed       | 224        |\n",
            "|    total_timesteps    | 42500      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.9      |\n",
            "|    explained_variance | -0.107     |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 8499       |\n",
            "|    policy_loss        | -74.3      |\n",
            "|    reward             | 0.89539856 |\n",
            "|    std                | 1.08       |\n",
            "|    value_loss         | 41.4       |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 189       |\n",
            "|    iterations         | 8600      |\n",
            "|    time_elapsed       | 226       |\n",
            "|    total_timesteps    | 43000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.9     |\n",
            "|    explained_variance | -0.186    |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 8599      |\n",
            "|    policy_loss        | 32.5      |\n",
            "|    reward             | -7.074416 |\n",
            "|    std                | 1.08      |\n",
            "|    value_loss         | 10.9      |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 189        |\n",
            "|    iterations         | 8700       |\n",
            "|    time_elapsed       | 229        |\n",
            "|    total_timesteps    | 43500      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.9      |\n",
            "|    explained_variance | -2.53      |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 8699       |\n",
            "|    policy_loss        | 27.3       |\n",
            "|    reward             | 0.25027725 |\n",
            "|    std                | 1.08       |\n",
            "|    value_loss         | 7.38       |\n",
            "--------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 189         |\n",
            "|    iterations         | 8800        |\n",
            "|    time_elapsed       | 232         |\n",
            "|    total_timesteps    | 44000       |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -11.9       |\n",
            "|    explained_variance | -0.328      |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 8799        |\n",
            "|    policy_loss        | -6.32       |\n",
            "|    reward             | 0.044426296 |\n",
            "|    std                | 1.08        |\n",
            "|    value_loss         | 0.658       |\n",
            "---------------------------------------\n",
            "----------------------------------------\n",
            "| time/                 |              |\n",
            "|    fps                | 188          |\n",
            "|    iterations         | 8900         |\n",
            "|    time_elapsed       | 235          |\n",
            "|    total_timesteps    | 44500        |\n",
            "| train/                |              |\n",
            "|    entropy_loss       | -11.9        |\n",
            "|    explained_variance | -0.139       |\n",
            "|    learning_rate      | 0.0007       |\n",
            "|    n_updates          | 8899         |\n",
            "|    policy_loss        | 18.9         |\n",
            "|    reward             | -0.010350997 |\n",
            "|    std                | 1.07         |\n",
            "|    value_loss         | 3.31         |\n",
            "----------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 189         |\n",
            "|    iterations         | 9000        |\n",
            "|    time_elapsed       | 238         |\n",
            "|    total_timesteps    | 45000       |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -11.9       |\n",
            "|    explained_variance | 0.0362      |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 8999        |\n",
            "|    policy_loss        | 7.16        |\n",
            "|    reward             | -0.23505999 |\n",
            "|    std                | 1.08        |\n",
            "|    value_loss         | 8.56        |\n",
            "---------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 189       |\n",
            "|    iterations         | 9100      |\n",
            "|    time_elapsed       | 240       |\n",
            "|    total_timesteps    | 45500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.9     |\n",
            "|    explained_variance | 0.659     |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 9099      |\n",
            "|    policy_loss        | 22.7      |\n",
            "|    reward             | 1.1207426 |\n",
            "|    std                | 1.08      |\n",
            "|    value_loss         | 4.55      |\n",
            "-------------------------------------\n",
            "----------------------------------------\n",
            "| time/                 |              |\n",
            "|    fps                | 189          |\n",
            "|    iterations         | 9200         |\n",
            "|    time_elapsed       | 242          |\n",
            "|    total_timesteps    | 46000        |\n",
            "| train/                |              |\n",
            "|    entropy_loss       | -11.9        |\n",
            "|    explained_variance | 0.0546       |\n",
            "|    learning_rate      | 0.0007       |\n",
            "|    n_updates          | 9199         |\n",
            "|    policy_loss        | -6.06        |\n",
            "|    reward             | -0.022654941 |\n",
            "|    std                | 1.07         |\n",
            "|    value_loss         | 7.56         |\n",
            "----------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 189        |\n",
            "|    iterations         | 9300       |\n",
            "|    time_elapsed       | 244        |\n",
            "|    total_timesteps    | 46500      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.9      |\n",
            "|    explained_variance | 0.0576     |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 9299       |\n",
            "|    policy_loss        | -39.3      |\n",
            "|    reward             | 0.34600133 |\n",
            "|    std                | 1.07       |\n",
            "|    value_loss         | 9.46       |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 189       |\n",
            "|    iterations         | 9400      |\n",
            "|    time_elapsed       | 248       |\n",
            "|    total_timesteps    | 47000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.9     |\n",
            "|    explained_variance | -0.028    |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 9399      |\n",
            "|    policy_loss        | 48.4      |\n",
            "|    reward             | -4.246097 |\n",
            "|    std                | 1.08      |\n",
            "|    value_loss         | 20.5      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 189       |\n",
            "|    iterations         | 9500      |\n",
            "|    time_elapsed       | 251       |\n",
            "|    total_timesteps    | 47500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.9     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 9499      |\n",
            "|    policy_loss        | -50.1     |\n",
            "|    reward             | -2.014636 |\n",
            "|    std                | 1.08      |\n",
            "|    value_loss         | 23.3      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 189       |\n",
            "|    iterations         | 9600      |\n",
            "|    time_elapsed       | 253       |\n",
            "|    total_timesteps    | 48000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12       |\n",
            "|    explained_variance | -0.045    |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 9599      |\n",
            "|    policy_loss        | -45.1     |\n",
            "|    reward             | 1.7801296 |\n",
            "|    std                | 1.08      |\n",
            "|    value_loss         | 21.9      |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 189        |\n",
            "|    iterations         | 9700       |\n",
            "|    time_elapsed       | 255        |\n",
            "|    total_timesteps    | 48500      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -12        |\n",
            "|    explained_variance | -0.0069    |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 9699       |\n",
            "|    policy_loss        | 22.4       |\n",
            "|    reward             | -1.0785043 |\n",
            "|    std                | 1.08       |\n",
            "|    value_loss         | 11.4       |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 189       |\n",
            "|    iterations         | 9800      |\n",
            "|    time_elapsed       | 258       |\n",
            "|    total_timesteps    | 49000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12       |\n",
            "|    explained_variance | -0.0101   |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 9799      |\n",
            "|    policy_loss        | -69.6     |\n",
            "|    reward             | 14.452761 |\n",
            "|    std                | 1.08      |\n",
            "|    value_loss         | 174       |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 189        |\n",
            "|    iterations         | 9900       |\n",
            "|    time_elapsed       | 261        |\n",
            "|    total_timesteps    | 49500      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -12        |\n",
            "|    explained_variance | -0.00399   |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 9899       |\n",
            "|    policy_loss        | -0.0208    |\n",
            "|    reward             | 0.29019484 |\n",
            "|    std                | 1.08       |\n",
            "|    value_loss         | 0.606      |\n",
            "--------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 189         |\n",
            "|    iterations         | 10000       |\n",
            "|    time_elapsed       | 264         |\n",
            "|    total_timesteps    | 50000       |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -12         |\n",
            "|    explained_variance | -0.105      |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 9999        |\n",
            "|    policy_loss        | 10.1        |\n",
            "|    reward             | 0.048462983 |\n",
            "|    std                | 1.09        |\n",
            "|    value_loss         | 1.71        |\n",
            "---------------------------------------\n",
            "------------------------------------\n",
            "| time/              |             |\n",
            "|    fps             | 262         |\n",
            "|    iterations      | 1           |\n",
            "|    time_elapsed    | 7           |\n",
            "|    total_timesteps | 2048        |\n",
            "| train/             |             |\n",
            "|    reward          | 0.051160906 |\n",
            "------------------------------------\n",
            "day: 2892, episode: 20\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 2992173.53\n",
            "total_reward: 1992173.53\n",
            "total_cost: 75143.48\n",
            "total_trades: 22807\n",
            "Sharpe: 0.735\n",
            "=================================\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 211          |\n",
            "|    iterations           | 2            |\n",
            "|    time_elapsed         | 19           |\n",
            "|    total_timesteps      | 4096         |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0060236193 |\n",
            "|    clip_fraction        | 0.0814       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -11.3        |\n",
            "|    explained_variance   | 0.000486     |\n",
            "|    learning_rate        | 0.00025      |\n",
            "|    loss                 | 1.81         |\n",
            "|    n_updates            | 10           |\n",
            "|    policy_gradient_loss | -0.00592     |\n",
            "|    reward               | 0.26281363   |\n",
            "|    std                  | 0.998        |\n",
            "|    value_loss           | 3.19         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 202          |\n",
            "|    iterations           | 3            |\n",
            "|    time_elapsed         | 30           |\n",
            "|    total_timesteps      | 6144         |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0077877715 |\n",
            "|    clip_fraction        | 0.0557       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -11.3        |\n",
            "|    explained_variance   | 0.000207     |\n",
            "|    learning_rate        | 0.00025      |\n",
            "|    loss                 | 3.84         |\n",
            "|    n_updates            | 20           |\n",
            "|    policy_gradient_loss | -0.0117      |\n",
            "|    reward               | -0.2627029   |\n",
            "|    std                  | 1            |\n",
            "|    value_loss           | 26.7         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 204          |\n",
            "|    iterations           | 4            |\n",
            "|    time_elapsed         | 39           |\n",
            "|    total_timesteps      | 8192         |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0066580293 |\n",
            "|    clip_fraction        | 0.0653       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -11.4        |\n",
            "|    explained_variance   | 0.0147       |\n",
            "|    learning_rate        | 0.00025      |\n",
            "|    loss                 | 4.1          |\n",
            "|    n_updates            | 30           |\n",
            "|    policy_gradient_loss | -0.00591     |\n",
            "|    reward               | 0.20669858   |\n",
            "|    std                  | 1            |\n",
            "|    value_loss           | 9.56         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 203          |\n",
            "|    iterations           | 5            |\n",
            "|    time_elapsed         | 50           |\n",
            "|    total_timesteps      | 10240        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0055411905 |\n",
            "|    clip_fraction        | 0.0701       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -11.3        |\n",
            "|    explained_variance   | 0.0311       |\n",
            "|    learning_rate        | 0.00025      |\n",
            "|    loss                 | 1.72         |\n",
            "|    n_updates            | 40           |\n",
            "|    policy_gradient_loss | -0.00482     |\n",
            "|    reward               | 1.9655452    |\n",
            "|    std                  | 0.994        |\n",
            "|    value_loss           | 4.2          |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 200         |\n",
            "|    iterations           | 6           |\n",
            "|    time_elapsed         | 61          |\n",
            "|    total_timesteps      | 12288       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.009818313 |\n",
            "|    clip_fraction        | 0.0737      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -11.3       |\n",
            "|    explained_variance   | -0.00358    |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 14.2        |\n",
            "|    n_updates            | 50          |\n",
            "|    policy_gradient_loss | -0.00863    |\n",
            "|    reward               | 0.44755548  |\n",
            "|    std                  | 0.996       |\n",
            "|    value_loss           | 24.5        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 199         |\n",
            "|    iterations           | 7           |\n",
            "|    time_elapsed         | 71          |\n",
            "|    total_timesteps      | 14336       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.009139852 |\n",
            "|    clip_fraction        | 0.082       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -11.3       |\n",
            "|    explained_variance   | 0.0159      |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 15.3        |\n",
            "|    n_updates            | 60          |\n",
            "|    policy_gradient_loss | -0.00647    |\n",
            "|    reward               | 1.5191044   |\n",
            "|    std                  | 0.996       |\n",
            "|    value_loss           | 44.5        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 201         |\n",
            "|    iterations           | 8           |\n",
            "|    time_elapsed         | 81          |\n",
            "|    total_timesteps      | 16384       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.009652865 |\n",
            "|    clip_fraction        | 0.0917      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -11.3       |\n",
            "|    explained_variance   | 0.0626      |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 10.5        |\n",
            "|    n_updates            | 70          |\n",
            "|    policy_gradient_loss | -0.009      |\n",
            "|    reward               | 0.10212509  |\n",
            "|    std                  | 0.997       |\n",
            "|    value_loss           | 20.4        |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 199          |\n",
            "|    iterations           | 9            |\n",
            "|    time_elapsed         | 92           |\n",
            "|    total_timesteps      | 18432        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0056049568 |\n",
            "|    clip_fraction        | 0.0968       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -11.3        |\n",
            "|    explained_variance   | 0.0429       |\n",
            "|    learning_rate        | 0.00025      |\n",
            "|    loss                 | 28.9         |\n",
            "|    n_updates            | 80           |\n",
            "|    policy_gradient_loss | -0.00908     |\n",
            "|    reward               | -1.0783684   |\n",
            "|    std                  | 0.998        |\n",
            "|    value_loss           | 35.1         |\n",
            "------------------------------------------\n",
            "----------------------------------------\n",
            "| time/                   |            |\n",
            "|    fps                  | 198        |\n",
            "|    iterations           | 10         |\n",
            "|    time_elapsed         | 103        |\n",
            "|    total_timesteps      | 20480      |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.0094916  |\n",
            "|    clip_fraction        | 0.116      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -11.3      |\n",
            "|    explained_variance   | 0.0737     |\n",
            "|    learning_rate        | 0.00025    |\n",
            "|    loss                 | 11.2       |\n",
            "|    n_updates            | 90         |\n",
            "|    policy_gradient_loss | -0.00995   |\n",
            "|    reward               | 0.07565315 |\n",
            "|    std                  | 1          |\n",
            "|    value_loss           | 62.4       |\n",
            "----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 200         |\n",
            "|    iterations           | 11          |\n",
            "|    time_elapsed         | 112         |\n",
            "|    total_timesteps      | 22528       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.010619672 |\n",
            "|    clip_fraction        | 0.0935      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -11.4       |\n",
            "|    explained_variance   | 0.00289     |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 65.5        |\n",
            "|    n_updates            | 100         |\n",
            "|    policy_gradient_loss | -0.011      |\n",
            "|    reward               | 3.7326748   |\n",
            "|    std                  | 1           |\n",
            "|    value_loss           | 102         |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 199          |\n",
            "|    iterations           | 12           |\n",
            "|    time_elapsed         | 122          |\n",
            "|    total_timesteps      | 24576        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.009738233  |\n",
            "|    clip_fraction        | 0.113        |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -11.4        |\n",
            "|    explained_variance   | 0.138        |\n",
            "|    learning_rate        | 0.00025      |\n",
            "|    loss                 | 5.97         |\n",
            "|    n_updates            | 110          |\n",
            "|    policy_gradient_loss | -0.00729     |\n",
            "|    reward               | -0.091475405 |\n",
            "|    std                  | 1            |\n",
            "|    value_loss           | 19.2         |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 198         |\n",
            "|    iterations           | 13          |\n",
            "|    time_elapsed         | 133         |\n",
            "|    total_timesteps      | 26624       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.011488937 |\n",
            "|    clip_fraction        | 0.0994      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -11.4       |\n",
            "|    explained_variance   | 0.0896      |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 26.5        |\n",
            "|    n_updates            | 120         |\n",
            "|    policy_gradient_loss | -0.00975    |\n",
            "|    reward               | 0.093595564 |\n",
            "|    std                  | 1           |\n",
            "|    value_loss           | 131         |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 198         |\n",
            "|    iterations           | 14          |\n",
            "|    time_elapsed         | 144         |\n",
            "|    total_timesteps      | 28672       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.009773454 |\n",
            "|    clip_fraction        | 0.0909      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -11.4       |\n",
            "|    explained_variance   | 0.151       |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 28.3        |\n",
            "|    n_updates            | 130         |\n",
            "|    policy_gradient_loss | -0.007      |\n",
            "|    reward               | -2.8825927  |\n",
            "|    std                  | 1.01        |\n",
            "|    value_loss           | 109         |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 199         |\n",
            "|    iterations           | 15          |\n",
            "|    time_elapsed         | 153         |\n",
            "|    total_timesteps      | 30720       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.011458829 |\n",
            "|    clip_fraction        | 0.135       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -11.4       |\n",
            "|    explained_variance   | 0.147       |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 19.6        |\n",
            "|    n_updates            | 140         |\n",
            "|    policy_gradient_loss | -0.0113     |\n",
            "|    reward               | 4.4676533   |\n",
            "|    std                  | 1           |\n",
            "|    value_loss           | 39.5        |\n",
            "-----------------------------------------\n",
            "day: 2892, episode: 30\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 6447599.85\n",
            "total_reward: 5447599.85\n",
            "total_cost: 68896.18\n",
            "total_trades: 22072\n",
            "Sharpe: 1.029\n",
            "=================================\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 198         |\n",
            "|    iterations           | 16          |\n",
            "|    time_elapsed         | 165         |\n",
            "|    total_timesteps      | 32768       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.010640651 |\n",
            "|    clip_fraction        | 0.144       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -11.4       |\n",
            "|    explained_variance   | 0.237       |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 33.5        |\n",
            "|    n_updates            | 150         |\n",
            "|    policy_gradient_loss | -0.00462    |\n",
            "|    reward               | 0.030541189 |\n",
            "|    std                  | 1.01        |\n",
            "|    value_loss           | 83.8        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 197         |\n",
            "|    iterations           | 17          |\n",
            "|    time_elapsed         | 176         |\n",
            "|    total_timesteps      | 34816       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.009165209 |\n",
            "|    clip_fraction        | 0.0816      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -11.4       |\n",
            "|    explained_variance   | 0.302       |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 76.9        |\n",
            "|    n_updates            | 160         |\n",
            "|    policy_gradient_loss | -0.00538    |\n",
            "|    reward               | -0.03633435 |\n",
            "|    std                  | 1.01        |\n",
            "|    value_loss           | 137         |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 198         |\n",
            "|    iterations           | 18          |\n",
            "|    time_elapsed         | 185         |\n",
            "|    total_timesteps      | 36864       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.014110316 |\n",
            "|    clip_fraction        | 0.127       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -11.4       |\n",
            "|    explained_variance   | 0.17        |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 47.7        |\n",
            "|    n_updates            | 170         |\n",
            "|    policy_gradient_loss | -0.0113     |\n",
            "|    reward               | 1.7247831   |\n",
            "|    std                  | 1.01        |\n",
            "|    value_loss           | 108         |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 198         |\n",
            "|    iterations           | 19          |\n",
            "|    time_elapsed         | 195         |\n",
            "|    total_timesteps      | 38912       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.011658663 |\n",
            "|    clip_fraction        | 0.126       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -11.4       |\n",
            "|    explained_variance   | 0.682       |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 8.76        |\n",
            "|    n_updates            | 180         |\n",
            "|    policy_gradient_loss | -0.00823    |\n",
            "|    reward               | -0.57265675 |\n",
            "|    std                  | 1.01        |\n",
            "|    value_loss           | 19.7        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 198         |\n",
            "|    iterations           | 20          |\n",
            "|    time_elapsed         | 206         |\n",
            "|    total_timesteps      | 40960       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.010882219 |\n",
            "|    clip_fraction        | 0.113       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -11.4       |\n",
            "|    explained_variance   | 0.274       |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 75.1        |\n",
            "|    n_updates            | 190         |\n",
            "|    policy_gradient_loss | -0.00864    |\n",
            "|    reward               | -0.58649737 |\n",
            "|    std                  | 1.01        |\n",
            "|    value_loss           | 170         |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 198         |\n",
            "|    iterations           | 21          |\n",
            "|    time_elapsed         | 217         |\n",
            "|    total_timesteps      | 43008       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.008119227 |\n",
            "|    clip_fraction        | 0.0874      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -11.5       |\n",
            "|    explained_variance   | 0.279       |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 103         |\n",
            "|    n_updates            | 200         |\n",
            "|    policy_gradient_loss | -0.00597    |\n",
            "|    reward               | -10.735629  |\n",
            "|    std                  | 1.01        |\n",
            "|    value_loss           | 165         |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 198         |\n",
            "|    iterations           | 22          |\n",
            "|    time_elapsed         | 226         |\n",
            "|    total_timesteps      | 45056       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.009943036 |\n",
            "|    clip_fraction        | 0.116       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -11.5       |\n",
            "|    explained_variance   | 0.167       |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 41          |\n",
            "|    n_updates            | 210         |\n",
            "|    policy_gradient_loss | -0.00884    |\n",
            "|    reward               | 4.304878    |\n",
            "|    std                  | 1.01        |\n",
            "|    value_loss           | 80.1        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 198         |\n",
            "|    iterations           | 23          |\n",
            "|    time_elapsed         | 237         |\n",
            "|    total_timesteps      | 47104       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.012381702 |\n",
            "|    clip_fraction        | 0.107       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -11.5       |\n",
            "|    explained_variance   | 0.221       |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 110         |\n",
            "|    n_updates            | 220         |\n",
            "|    policy_gradient_loss | -0.00894    |\n",
            "|    reward               | 1.3568928   |\n",
            "|    std                  | 1.01        |\n",
            "|    value_loss           | 185         |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 197         |\n",
            "|    iterations           | 24          |\n",
            "|    time_elapsed         | 248         |\n",
            "|    total_timesteps      | 49152       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.010439983 |\n",
            "|    clip_fraction        | 0.0981      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -11.5       |\n",
            "|    explained_variance   | 0.256       |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 82.5        |\n",
            "|    n_updates            | 230         |\n",
            "|    policy_gradient_loss | -0.00797    |\n",
            "|    reward               | 13.046707   |\n",
            "|    std                  | 1.01        |\n",
            "|    value_loss           | 139         |\n",
            "-----------------------------------------\n",
            "----------------------------------------\n",
            "| time/                   |            |\n",
            "|    fps                  | 198        |\n",
            "|    iterations           | 25         |\n",
            "|    time_elapsed         | 257        |\n",
            "|    total_timesteps      | 51200      |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.0106542  |\n",
            "|    clip_fraction        | 0.125      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -11.5      |\n",
            "|    explained_variance   | 0.0678     |\n",
            "|    learning_rate        | 0.00025    |\n",
            "|    loss                 | 85.8       |\n",
            "|    n_updates            | 240        |\n",
            "|    policy_gradient_loss | -0.00805   |\n",
            "|    reward               | 0.78078485 |\n",
            "|    std                  | 1.01       |\n",
            "|    value_loss           | 132        |\n",
            "----------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trained_a2c.save(TRAINED_MODEL_DIR + \"/agent_a2c\") if if_using_a2c else None\n",
        "trained_ppo.save(TRAINED_MODEL_DIR + \"/agent_ppo\") if if_using_ppo else None"
      ],
      "metadata": {
        "id": "D-5pDMVwEbzN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}